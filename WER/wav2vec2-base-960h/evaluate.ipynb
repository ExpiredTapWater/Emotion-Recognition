{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c9b5c9",
   "metadata": {
    "id": "e36c34af-36e8-483f-90dd-bc10634ebbd2",
    "tags": []
   },
   "source": [
    "## Evaluate.ipynb\n",
    "#### **This notebook evaluates the accuracy of just one model on the IEMOCAP dataset ground truths.**\n",
    "\n",
    "- **Transcription Model (S2T):** wav2vec2-base-960h\n",
    "- Source: [HuggingFace](https://huggingface.co/facebook/wav2vec2-base-960h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d5995-95fe-4940-b295-6ab91c446e5c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82dda0d5-0c4e-4cd6-b171-3d441877cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "SEED = 22\n",
    "FOLD = 0\n",
    "\n",
    "# Models\n",
    "TRANSCRIPTION_MODEL_NAME = \"facebook/wav2vec2-base-960h\"\n",
    "\n",
    "# Flag to enable parsing of arguments when converted to script. Set true after converting\n",
    "PYTHON_SCRIPT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81236a9-d30f-475d-98d0-6f5c47ed96eb",
   "metadata": {},
   "source": [
    "### For Conversion to .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8f7109-dbdb-4350-8590-662f813be489",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTHON_SCRIPT:\n",
    "\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"\")\n",
    "    parser.add_argument('--seed',default=2021,type=int)\n",
    "    parser.add_argument('--fold',type=int,required=True)\n",
    "    parser.add_argument('--remap',type=bool,required=True)\n",
    "    parser.add_argument('--threshold',type=float,required=False)\n",
    "    parser.add_argument('--mode',required=False)\n",
    "    parser.add_argument('--flip',type=bool,required=False)\n",
    "\n",
    "    # Parse the arguments\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Run\n",
    "    SEED = args.seed\n",
    "    FOLD = args.fold\n",
    "    RUN_REMAP = args.remap\n",
    "    THRESHOLD = args.threshold\n",
    "    MODE = args.mode\n",
    "    FLIP = args.flip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e1bde-c41d-4cd2-a994-152060ad0a03",
   "metadata": {},
   "source": [
    "### Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c23c382c-733f-4299-b748-a92b760bdfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "OUTPUT_FOLDER = f'./fold_{FOLD}'\n",
    "LOG_OUTPUT = OUTPUT_FOLDER + f'/fold-{FOLD}.log'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0f670-50ec-4ede-b8bc-0b2af067a327",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f710661d-9e56-4137-9ed1-f96b8e39f09b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "TSV = r'C:\\Users\\ChenYi\\Downloads\\AAI3001_Project\\labels\\IEMOCAP_4.tsv'\n",
    "AUDIO_DIRECTORY = r'C:\\Users\\ChenYi\\Downloads\\AAI3001_Project\\small-project\\IEMOCAP_full_release_audio'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80069643-074b-4084-bcf5-d6bd2393cc08",
   "metadata": {},
   "source": [
    "### Select GPUs (For multi-GPU setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "914881e1-ae8c-45a2-9b85-0140639b3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ef3cb-d36c-49fe-ae9a-7321ca2945b5",
   "metadata": {},
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd59e8e-ec78-4775-9abb-4c599e79306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def get_logger(filename, verbosity=1, name=None):\n",
    "    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n",
    "    formatter = logging.Formatter(\"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\")\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level_dict[verbosity])\n",
    "\n",
    "    fh = logging.FileHandler(filename, \"w\")\n",
    "    fh.setFormatter(formatter)                                                                                                                                                                                     \n",
    "    logger.addHandler(fh)                                                                                                                                                                                          \n",
    "                                                                                                                                                                                                                   \n",
    "    sh = logging.StreamHandler()                                                                                                                                                                                   \n",
    "    sh.setFormatter(formatter)                                                                                                                                                                                     \n",
    "    logger.addHandler(sh)                                                                                                                                                                                          \n",
    "                                                                                                                                                                                                                   \n",
    "    return logger\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "logger = get_logger(LOG_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb00abe-4faf-45f1-a0c3-28c6172de5d1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e6792e-53d3-4af3-af88-2da33070c734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-01-12 03:05:56,979][config.py][line:54][INFO] PyTorch version 2.4.1+cu124 available.\n",
      "[2025-01-12 03:05:56,981][config.py][line:101][INFO] TensorFlow version 2.18.0 available.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import librosa\n",
    "import torchaudio\n",
    "import contractions\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torchaudio import functional as audioF\n",
    "from torchaudio.transforms import Resample\n",
    "from torchaudio.compliance import kaldi\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2FeatureExtractor, pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b93704-f348-4aaf-8bf1-18fd7237bc0d",
   "metadata": {},
   "source": [
    "### Log Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86eaf711-220a-46fa-967c-ca03246db008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-01-12 03:06:31,851][1971636866.py][line:1][INFO] ----- Models -----\n",
      "[2025-01-12 03:06:31,852][1971636866.py][line:2][INFO] Speech-To-Text (Transcription) Model: facebook/wav2vec2-base-960h\n",
      "[2025-01-12 03:06:31,853][1971636866.py][line:3][INFO] ----- Parameters -----\n",
      "[2025-01-12 03:06:31,853][1971636866.py][line:4][INFO] Seed: 22\n",
      "[2025-01-12 03:06:31,854][1971636866.py][line:5][INFO] Fold: 0\n",
      "[2025-01-12 03:06:31,855][1971636866.py][line:6][INFO] --------------------\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"----- Models -----\")\n",
    "logger.info(f\"Speech-To-Text (Transcription) Model: {TRANSCRIPTION_MODEL_NAME}\")\n",
    "logger.info(\"----- Parameters -----\")\n",
    "logger.info(f\"Seed: {SEED}\")\n",
    "logger.info(f\"Fold: {FOLD}\")\n",
    "logger.info(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e050c689-e0ec-4a01-8eb2-c38c7222069c",
   "metadata": {},
   "source": [
    "### Provided Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "273aedcd-5423-4f3b-bddc-530ee08dfb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pad_trunc_wav(nn.Module):\n",
    "    def __init__(self, max_len: int = 6*16000):\n",
    "        super(Pad_trunc_wav, self).__init__()\n",
    "        self.max_len = max_len\n",
    "    def forward(self,x):\n",
    "        shape = x.shape\n",
    "        length = shape[1]\n",
    "        if length < self.max_len:\n",
    "            multiple = self.max_len//length+1\n",
    "            x_tmp = torch.cat((x,)*multiple, axis=1)\n",
    "            x_new = x_tmp[:,0:self.max_len]\n",
    "        else:\n",
    "            x_new = x[:,0:self.max_len]\n",
    "        return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90c61650-5d76-4bde-bae6-26ef3f8e75ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed=2021):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "setup_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882d066-c8a2-455b-92d3-d34639944e33",
   "metadata": {},
   "source": [
    "### Download Required Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "793e77c7-fe84-439b-8380-8a9c280d2f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChenYi\\anaconda3\\envs\\aai3001\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2025-01-12 03:06:36,441][3337139990.py][line:7][INFO] Speech-To-Text (Transcription) model loaded from facebook/wav2vec2-base-960h successfully\n"
     ]
    }
   ],
   "source": [
    "# Load Wav2Vec2 model and processor for speech-to-text\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(TRANSCRIPTION_MODEL_NAME)\n",
    "S2T_processor = Wav2Vec2Processor.from_pretrained(TRANSCRIPTION_MODEL_NAME)\n",
    "S2T_Model = Wav2Vec2ForCTC.from_pretrained(TRANSCRIPTION_MODEL_NAME)\n",
    "S2T_Model = S2T_Model.to('cuda')\n",
    "\n",
    "logger.info(F\"Speech-To-Text (Transcription) model loaded from {TRANSCRIPTION_MODEL_NAME} successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d267a401-d541-40ca-8dfd-a44b1abdd79a",
   "metadata": {},
   "source": [
    "### Dataset & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c61df84-9d17-4a42-a82c-7881921b8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydataset(Dataset):\n",
    "    def __init__(self, mode='train', max_len=6, seed=2021, fold=0, data_path=TSV, audio_dir=AUDIO_DIRECTORY):\n",
    "        self.mode = mode\n",
    "        data_all = pd.read_csv(data_path, sep='\\t')\n",
    "        SpkNames = np.unique(data_all['speaker'])  # ['Ses01F', 'Ses01M', ..., 'Ses05M']\n",
    "        self.data_info = self.split_dataset(data_all, fold, SpkNames, mode)\n",
    "        self.get_audio_dir_path = os.path.join(audio_dir)\n",
    "        self.pad_trunc = Pad_trunc_wav(max_len * 16000)\n",
    "         \n",
    "        # Label encoding\n",
    "        self.label = self.data_info['label'].astype('category').cat.codes.values\n",
    "        self.ClassNames = np.unique(self.data_info['label'])\n",
    "        self.NumClasses = len(self.ClassNames)\n",
    "        self.weight = 1 / torch.tensor([(self.label == i).sum() for i in range(self.NumClasses)]).float()\n",
    "\n",
    "    def get_classname(self):\n",
    "        return self.ClassNames\n",
    "    \n",
    "    # Updated split_dataset function using fold\n",
    "    \n",
    "    def split_dataset(self, df_all, fold, speakers, mode):\n",
    "        \n",
    "        spk_len = len(speakers)\n",
    "        test_idx = np.array(df_all['speaker']==speakers[fold%spk_len])\n",
    "        if fold%2==0:\n",
    "            val_idx = np.array(df_all['speaker']==speakers[(fold+1)%spk_len])\n",
    "        else:\n",
    "            val_idx = np.array(df_all['speaker']==speakers[(fold-1)%spk_len])\n",
    "        train_idx = True^(test_idx+val_idx)\n",
    "        train_data_info = df_all[train_idx].reset_index(drop=True)\n",
    "        val_data_info = df_all[val_idx].reset_index(drop=True)\n",
    "        test_data_info = df_all[test_idx].reset_index(drop=True)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            data_info = train_data_info\n",
    "        elif self.mode == 'val':\n",
    "            data_info = val_data_info\n",
    "        elif self.mode == 'test':\n",
    "            data_info = test_data_info\n",
    "        else:\n",
    "            data_info = df_all\n",
    "        \n",
    "        logger.info(f\"Mode: {mode} Fold: {fold}\")\n",
    "        return data_info\n",
    "\n",
    "    def pre_process(self, wav):\n",
    "        \n",
    "        if self.mode == 'test': \n",
    "            return wav\n",
    "        else:\n",
    "            wav = self.pad_trunc(wav)\n",
    "            return wav\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the raw waveform from file using data_info to get filenames\n",
    "        wav_path = os.path.join(self.get_audio_dir_path, self.data_info['filename'][idx]) + '.wav'\n",
    "        wav, sample_rate = torchaudio.load(wav_path)\n",
    "\n",
    "        # Preprocess the waveform (e.g., pad/truncate if needed)\n",
    "        wav = self.pre_process(wav)\n",
    "\n",
    "        # Apply Wav2Vec2 feature extractor\n",
    "        inputs = feature_extractor(\n",
    "            wav.squeeze().numpy(),  # Convert PyTorch tensor to numpy array\n",
    "            sampling_rate=sample_rate,\n",
    "            return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "            padding=True  # Optionally pad to a fixed length\n",
    "        )\n",
    "\n",
    "        label = self.label[idx]\n",
    "\n",
    "        # Return the processed input values and the label\n",
    "        return {\n",
    "            'input_values': inputs['input_values'].squeeze(0),  # Remove extra batch dimension\n",
    "            'labels': torch.tensor(label, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81905e24-6e30-4ac3-9187-132aab58d7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-01-12 03:06:39,448][1700717295.py][line:43][INFO] Mode: test Fold: 0\n",
      "[2025-01-12 03:06:39,455][1146238807.py][line:4][INFO] Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Instantiate datasets\n",
    "test_dataset = Mydataset(mode='test', max_len=6, fold=FOLD)\n",
    "\n",
    "logger.info(\"Dataset Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc062c2-432e-4e44-9cc8-d51339176b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put test information into a dataframe for later use\n",
    "data_info = test_dataset.data_info\n",
    "test_dataframe = data_info[['filename', 'label']].copy()\n",
    "test_dataframe['filepath'] = test_dataframe['filename'].apply(\n",
    "    lambda x: os.path.join(test_dataset.get_audio_dir_path, f\"{x}.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b530b91c-7c90-42b7-a270-9ecd3f831a7a",
   "metadata": {},
   "source": [
    "## Perform Inference and Obtain Transcriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa70b8-021a-4ec5-98ca-f282e2e470ef",
   "metadata": {},
   "source": [
    "#### Function to clean up transcription to match what is required for compute_wer.py\n",
    "- Remove full stop\n",
    "- Remove all punctuations except apostrophe\n",
    "- Change all to UPPERCASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec548ad2-b17a-42cd-9de5-73f06f24231b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_transcription(ID, text):\n",
    "    \n",
    "    original_text = text  # Store the original text for comparison\n",
    "    \n",
    "    # Remove full stop\n",
    "    text = text.replace('.', '')\n",
    "    \n",
    "    # Remove all punctuations except apostrophe\n",
    "    text = ''.join(char if char.isalnum() or char == \"'\" else ' ' for char in text)\n",
    "    \n",
    "    # Convert all text to uppercase\n",
    "    text = text.upper()\n",
    "\n",
    "    # Alert if any formatting was done:\n",
    "    if text != original_text:\n",
    "        logger.info(f\"Transcription of file {ID} has been formatted\")\n",
    "        logger.info(f\"ORIGINAL: {original_text}\")\n",
    "        logger.info(f\"FORMATTED: {text}\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50d944a7-561c-4c82-88c9-a5843f4b3b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_dataframe):\n",
    "\n",
    "    results = []\n",
    "    total = test_dataframe.shape[0]\n",
    "    count = 1\n",
    "\n",
    "    # Iterate over each audio file in the test folder\n",
    "    for index, row in test_dataframe.iterrows():\n",
    "\n",
    "        # Display progress\n",
    "        print(f'File {count} of {total}', end='\\r')\n",
    "        count += 1\n",
    "\n",
    "        # Load audio file\n",
    "        filename = row['filename'] + '.wav'\n",
    "        audio_file = os.path.join(AUDIO_DIRECTORY, filename)\n",
    "        audio, sample_rate = librosa.load(audio_file, sr = 16000)\n",
    "        \n",
    "        # Tokenize the input audio for speech-to-text model\n",
    "        input_values = S2T_processor(audio, return_tensors=\"pt\", sampling_rate=16000, padding=\"do_not_pad\").input_values\n",
    "        input_values = input_values.to('cuda')\n",
    "        \n",
    "        # Important!\n",
    "        # Batch size should be 1 for inference. I am not using a dataloader, and iterating per file, so my batch size is effectively one\n",
    "        # Padding=\"longest\" pads each file to the longest in each batch, but because my batch is 1, effectively no padding is done.\n",
    "        # For clarify, padding=\"do_not_pad\" is used\n",
    "        \n",
    "        # Obtain transcriptions\n",
    "        with torch.no_grad():\n",
    "            S2T_logits = S2T_Model(input_values).logits\n",
    "            predicted_ids = torch.argmax(S2T_logits, dim=-1)\n",
    "            transcription = S2T_processor.batch_decode(predicted_ids)[0]\n",
    "            \n",
    "        # Perform cleaning\n",
    "        formatted_transcription = format_transcription(count, transcription)\n",
    "\n",
    "        # Extract the filename without the extension\n",
    "        filename = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "\n",
    "        # Append the result to the list\n",
    "        results.append([filename,\n",
    "                        formatted_transcription])\n",
    "\n",
    "    logger.info(f\"Done processing {total} files\")\n",
    "\n",
    "    # Write the results to a CSV file\n",
    "    global CSV_FILEPATH\n",
    "    CSV_FILEPATH = os.path.join(OUTPUT_FOLDER, \"transcriptions.csv\")\n",
    "\n",
    "    with open(CSV_FILEPATH, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['ID', 'transcription'])\n",
    "        writer.writerows(results)\n",
    "\n",
    "    logger.info(f\"Transcriptions saved to {CSV_FILEPATH}\")\n",
    "    return CSV_FILEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d251a5c1-56c8-4e69-a07c-ba5d4672ee46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 515 of 528\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-01-12 03:26:14,412][2615092208.py][line:44][INFO] Done processing 528 files\n",
      "[2025-01-12 03:26:14,415][2615092208.py][line:55][INFO] Transcriptions saved to ./fold_0\\transcriptions.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 528 of 528\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./fold_0\\\\transcriptions.csv'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(test_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca6e4d-8bcf-4a31-82d9-81a559502279",
   "metadata": {},
   "source": [
    "## Convert CSV into TXT file\n",
    "- Optional step to convert into TXT file for use in compute_wer.py\n",
    "- This function can also be found in notebooks/TranscriptionFormatter.ipynb, where you can optionaly convert each fold at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bc49d59-cbca-4eb4-86fb-8cfd0ca9e731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input and output file paths\n",
    "input_csv = CSV_FILEPATH  # Replace with your CSV file path\n",
    "output_txt = OUTPUT_FOLDER + \"/transcriptions.txt\"  # Replace with your desired TXT file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26b55b66-748b-4b51-8963-d3654712af37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted and formatted ./fold_0\\transcriptions.csv to ./fold_0/transcriptions.txt for use in computing word error rate.\n"
     ]
    }
   ],
   "source": [
    "# Open the CSV file and read its content\n",
    "def convert_csv_to_txt(input_csv, output_txt):\n",
    "    with open(input_csv, mode=\"r\", encoding=\"utf-8\") as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        header = next(csv_reader, None)  # Skip header if present\n",
    "        with open(output_txt, mode=\"w\", encoding=\"utf-8\") as txt_file:\n",
    "            for row in csv_reader:\n",
    "                # Combine ID and Transcript with a space separator\n",
    "                txt_file.write(f\"{row[0]} {row[1]}\\n\")\n",
    "    \n",
    "convert_csv_to_txt(input_csv, output_txt)\n",
    "print(f\"Converted and formatted {input_csv} to {output_txt} for use in computing word error rate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056fcc4c-fc56-449e-84b3-a9e941bd1eda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5782060,
     "sourceId": 9500890,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "AAI3001",
   "language": "python",
   "name": "aai3001"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
